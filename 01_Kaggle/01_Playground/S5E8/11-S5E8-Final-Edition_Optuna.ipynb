{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"},{"sourceId":9293783,"sourceType":"datasetVersion","datasetId":5626665}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 通常の pandas 動作が倍速になる\n%load_ext cudf.pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:07.214837Z","iopub.execute_input":"2025-09-15T01:28:07.215438Z","iopub.status.idle":"2025-09-15T01:28:17.926735Z","shell.execute_reply.started":"2025-09-15T01:28:07.215418Z","shell.execute_reply":"2025-09-15T01:28:17.925909Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【データのインポート】","metadata":{}},{"cell_type":"code","source":"# ライブラリのインポート\nimport numpy as np\nimport pandas as pd\nimport os\nimport cudf\nfrom cuml.preprocessing import TargetEncoder\n\n# データフレーム読み込み\ntrain_df = pd.read_csv(\"/kaggle/input/playground-series-s5e8/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s5e8/test.csv\")\n\n# 外部データセットの追加\norig_df = pd.read_csv(\"/kaggle/input/bank-marketing-dataset-full/bank-full.csv\",delimiter=\";\")\norig_df['y'] = orig_df.y.map({'yes':1,'no':0})\norig_df['id'] = (np.arange(len(orig_df))+1e6).astype('int')\norig_df = orig_df.set_index('id')\n\n# データ結合\nall_df = pd.concat([train_df,test_df,orig_df],axis=0,ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:17.928187Z","iopub.execute_input":"2025-09-15T01:28:17.928629Z","iopub.status.idle":"2025-09-15T01:28:23.982587Z","shell.execute_reply.started":"2025-09-15T01:28:17.928610Z","shell.execute_reply":"2025-09-15T01:28:23.982051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【前処理】","metadata":{}},{"cell_type":"code","source":"# カテゴリ列名と数値列名の取得\ndef preprocess1(df):\n    CATS = []\n    NUMS = []\n    for c in (df.drop([\"id\",\"y\"],axis=1)).columns:\n        t = \"CAT\"\n        if df[c].dtype=='object':\n            CATS.append(c)\n        else:\n            NUMS.append(c)\n            t = \"NUM\"\n        n = df[c].nunique()\n        na = df[c].isna().sum()\n        # print(f\"[{t}] {c} has {n} unique and {na} NA\")\n    # print(\"CATS:\", CATS )\n    # print(\"NUMS:\", NUMS )\n    return df, CATS, NUMS\n\n# 内部データと外部データに適用\nCATS = []\nNUMS = []\nall_df, CATS, NUMS = preprocess1(all_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:23.983349Z","iopub.execute_input":"2025-09-15T01:28:23.983773Z","iopub.status.idle":"2025-09-15T01:28:24.118589Z","shell.execute_reply.started":"2025-09-15T01:28:23.983753Z","shell.execute_reply":"2025-09-15T01:28:24.117995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ラベルエンコードとユニーク数の取得\ndef preprocess2(df, CATS, NUMS):\n    # NUMS：数値列、NUMS2：カテゴリ数、CATS1：カテゴリ数\n    CATS1 = [] # 数値列\n    SIZES = {} # カテゴリ数\n\n    for c in NUMS + CATS:\n        n = c\n        # 数値列のとき\n        if c in NUMS: \n            n = f\"{c}2\"\n            CATS1.append(n)\n        # カテゴリ列のとき、ラベルエンコード\n        df[n], uniques = df[c].factorize()\n        # カテゴリ数\n        SIZES[n] = len(uniques)\n        # print(c)\n        df[c] = df[c].astype('int32')\n        df[n] = df[n].astype('int32')\n\n    # print(\"New CATS:\", CATS1 )\n    # print(\"Cardinality of all CATS:\", SIZES )\n    return df, CATS1, SIZES\n\n# 内部データと外部データに適用\nCATS1 = []\nSIZES = []\nall_df, CATS1, SIZES = preprocess2(all_df, CATS, NUMS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:24.119318Z","iopub.execute_input":"2025-09-15T01:28:24.119552Z","iopub.status.idle":"2025-09-15T01:28:24.375747Z","shell.execute_reply.started":"2025-09-15T01:28:24.119525Z","shell.execute_reply":"2025-09-15T01:28:24.375244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from itertools import combinations\n\n# カラムペアの作成(カラムxユニーク数+カラム)\ndef preprocess3(df, CATS, CATS1, SIZES):\n    pairs = combinations(CATS + CATS1, 2)\n    new_cols = {}\n    CATS2 = []\n\n    for c1, c2 in pairs:\n        name = \"_\".join(sorted((c1, c2)))\n        new_cols[name] = df[c1] * SIZES[c2] + df[c2]\n        CATS2.append(name)\n    if new_cols:\n        new_df = pd.DataFrame(new_cols)         \n        df = pd.concat([df, new_df], axis=1) \n\n    # print(f\"Created {len(CATS2)} new CAT columns\")\n    return df, CATS2\n\n# 内部データと外部データに適用\nCATS2 = []\nall_df, CATS2 = preprocess3(all_df, CATS, CATS1, SIZES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:24.377336Z","iopub.execute_input":"2025-09-15T01:28:24.377545Z","iopub.status.idle":"2025-09-15T01:28:24.627812Z","shell.execute_reply.started":"2025-09-15T01:28:24.377528Z","shell.execute_reply":"2025-09-15T01:28:24.627255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# カウントエンコード\ndef preprocess4(df, CATS, CATS1, CATS2):\n    \n    CC = CATS+CATS1+CATS2\n\n    print(f\"Processing {len(CC)} columns... \",end=\"\")\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n        tmp = df.groupby(c).y.count()\n        tmp = tmp.astype('int32')\n        tmp.name = f\"CE_{c}\"\n        CE.append( f\"CE_{c}\" )\n        df = df.merge(tmp, on=c, how='left')\n    print()\n    return df, CE\n\n# 内部データと外部データに適用\nCE = []\nall_df, CE = preprocess4(all_df, CATS, CATS1, CATS2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:24.628616Z","iopub.execute_input":"2025-09-15T01:28:24.628893Z","iopub.status.idle":"2025-09-15T01:28:35.086894Z","shell.execute_reply.started":"2025-09-15T01:28:24.628865Z","shell.execute_reply":"2025-09-15T01:28:35.086257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# yがint64なのでint32に変換する(GPU対応のため)\nall_df[\"y\"] = all_df[\"y\"].astype(\"int32\")\n\n# データ分割\ntrain1 = all_df.iloc[:len(train_df)]\ntest1 = all_df.iloc[len(train_df):len(train_df)+len(test_df)]\n\n# 外部データセットあり\norig = all_df.iloc[-len(orig_df):]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:29:41.421485Z","iopub.execute_input":"2025-09-15T01:29:41.421747Z","iopub.status.idle":"2025-09-15T01:29:41.461089Z","shell.execute_reply.started":"2025-09-15T01:29:41.421727Z","shell.execute_reply":"2025-09-15T01:29:41.460527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess5(train, test, CATS, CATS1, CATS2, orig):\n\n    CC = CATS+CATS1+CATS2\n\n    print(f\"Processing {len(CC)} columns... \",end=\"\")\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n        tmp = orig.groupby(c).y.mean()\n        tmp = tmp.astype('float32')\n        tmp.name = f\"TE_ORIG_{c}\"\n        TE_ORIG.append( f\"TE_ORIG_{c}\" )\n        train = train.merge(tmp, on=c, how='left')\n        test = test.merge(tmp, on=c, how='left')\n    return train, test\n    print()\n\nTE_ORIG = []\ntrain2, test2 = preprocess5(train1, test1, CATS, CATS1, CATS2, orig)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:33:50.431803Z","iopub.execute_input":"2025-09-15T01:33:50.432513Z","iopub.status.idle":"2025-09-15T01:34:17.630941Z","shell.execute_reply.started":"2025-09-15T01:33:50.432485Z","shell.execute_reply":"2025-09-15T01:34:17.630169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【Light GBM + Optuna】","metadata":{}},{"cell_type":"code","source":"# ###########################################\n# ############ Light GBM + Optuna ###########\n# ###########################################\n# import optuna\n# import numpy as np\n# import pandas as pd\n# import lightgbm as lgb\n# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import StratifiedKFold, KFold\n# # from category_encoders import TargetEncoder # CPU版\n# from cuml.preprocessing import TargetEncoder\n# import cudf\n# import warnings\n# import gc\n# warnings.filterwarnings(\"ignore\")\n\n# # 入力データ\n# X = train1.drop([\"id\",\"y\"], axis=1).copy()\n# y = train1[\"y\"].copy()\n\n# def objective(trial):\n\n#     # 整数は±50%、確率系は±0.1〜0.2、正則化は対数で±1〜2\n#     # LightGBMパラメータ\n#     lgbm_params = {\n#         'objective': 'binary',\n#         'device': 'gpu',\n#         'metric': 'auc',\n#         'boosting_type': 'gbdt',\n#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 52, 116),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 10, 15),\n#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 15, 35),\n#         \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.0, 0.38),\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.55, 1.0),\n#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.73, 1.0),\n#         \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 5),\n#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10),\n#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 8),\n#         \"min_sum_hessian_in_leaf\": trial.suggest_float(\"min_sum_hessian_in_leaf\", 1e-3, 4),\n#         'verbosity': -1\n#     }\n\n#     pred_lgb = np.zeros(len(X))\n#     fold_scores = []\n\n#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n#     # kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n#     # Fold分割し格納\n#     for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n#         print(\"#\"*25)\n#         print(f\"### Fold {fold+1}\")\n#         print(\"#\"*25)\n\n#         X_train = X.iloc[train_idx,:].copy()\n#         y_train = y.iloc[train_idx].copy()\n#         X_valid = X.iloc[valid_idx,:].copy()\n#         y_valid = y.iloc[valid_idx].copy()\n\n#         # ターゲットエンコーディング\n#         CC = CATS1_in+CATS2_in\n#         print(f\"Target encoding {len(CC)} features... \",end=\"\")\n\n#         # グローバル平均\n#         # global_mean = y_train.mean()\n    \n#         for i,c in enumerate(CC):\n#             if i%10==0: print(f\"{i}, \",end=\"\")\n            \n#             TE0 = TargetEncoder(n_folds=5, smooth=10, \n#                                 split_method='random', stat='mean')\n#             X_train[c] = TE0.fit_transform(X_train[c],y_train).astype('float32')\n#             # X_valid[c] = np.where(np.isnan(X_valid[c]), global_mean, X_valid[c])\n#             X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        \n#         print()\n\n#         # TE0を明示的に削除\n#         del TE0\n#         gc.collect()\n\n#         # CC以外はカテゴリ型に変換\n#         for c in CATS_in:\n#             if c not in CC:  \n#                 X_train[c] = X_train[c].astype('category')\n#                 X_valid[c] = X_valid[c].astype('category')\n\n#         # データセット\n#         lgb_train = lgb.Dataset(\n#             X_train,y_train,categorical_feature=CATS_in)\n\n#         lgb_valid = lgb.Dataset(\n#             X_valid,y_valid,categorical_feature=CATS_in)\n#         # --------------------------\n#         # 学習\n#         # --------------------------\n#         model_lgb = lgb.train(\n#             lgbm_params,\n#             lgb_train,\n#             num_boost_round=2000,\n#             valid_sets=[lgb_train, lgb_valid],\n#             valid_names=[\"train\", \"valid\"],\n#             callbacks=[\n#                 lgb.early_stopping(stopping_rounds=100, verbose=False),\n#                 lgb.log_evaluation(500),\n#             ])\n\n#         # 予測\n#         pred_lgb[valid_idx] = model_lgb.predict(\n#             X_valid, num_iteration=model_lgb.best_iteration)\n\n#         # AUCスコア算出\n#         fold_scores.append(roc_auc_score(y_valid,pred_lgb[valid_idx]))\n\n#     # 各Foldの平均値\n#     score = np.mean(fold_scores)\n    \n#     return score\n\n# # 最適化実行\n# study = optuna.create_study(direction=\"maximize\",\n#                            pruner=optuna.pruners.MedianPruner())\n# study.optimize(objective, n_trials=12)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.349450Z","iopub.status.idle":"2025-09-15T01:28:35.349781Z","shell.execute_reply.started":"2025-09-15T01:28:35.349614Z","shell.execute_reply":"2025-09-15T01:28:35.349628Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 最もスコアが高かった Trial\n# best_trial = study.best_trial\n# print(\"Best AUC:\", best_trial.value)\n# print(\"Best parameters:\")\n# for key, val in best_trial.params.items():\n#     print(f\"  {key}: {val}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.350554Z","iopub.status.idle":"2025-09-15T01:28:35.350829Z","shell.execute_reply.started":"2025-09-15T01:28:35.350692Z","shell.execute_reply":"2025-09-15T01:28:35.350705Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Trialをスコア順にソートして上位5件を表示\n# trials_df = study.trials_dataframe()\n# trials_df = trials_df.sort_values('value', ascending=False)\n# print(trials_df.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.351811Z","iopub.status.idle":"2025-09-15T01:28:35.352021Z","shell.execute_reply.started":"2025-09-15T01:28:35.351923Z","shell.execute_reply":"2025-09-15T01:28:35.351931Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# optuna.visualization.plot_param_importances(study)  # パラメータ重要度\n# optuna.visualization.plot_optimization_history(study)  # スコア推移","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.353384Z","iopub.status.idle":"2025-09-15T01:28:35.353618Z","shell.execute_reply.started":"2025-09-15T01:28:35.353504Z","shell.execute_reply":"2025-09-15T01:28:35.353516Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for i, trial in enumerate(study.best_trials):\n#   print(trial.params)\n#   print([j for j in trial.values])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.355466Z","iopub.status.idle":"2025-09-15T01:28:35.355762Z","shell.execute_reply.started":"2025-09-15T01:28:35.355634Z","shell.execute_reply":"2025-09-15T01:28:35.355648Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【① Light GBM (内部データセットのみ)】","metadata":{}},{"cell_type":"code","source":"############################################################\n############ Light GBM with OOF Target Encoding ############\n############################################################\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\n# from category_encoders import TargetEncoder # CPU版\nfrom cuml.preprocessing import TargetEncoder\nimport cudf\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")\n\n# 学習、バリデーションデータ\npred_lgb1 = np.zeros(len(train1))\npred_lgb_test1 = np.zeros(len(test1))\nmodels_lgb1 = []\n\n# 入力データ\nX = train1.drop([\"id\",\"y\"], axis=1).copy()\ny = train1[\"y\"].copy()\ntest_ = test1.drop([\"id\",\"y\"], axis=1).copy()\n\n# LightGBMパラメータ\nlgbm_params = {\n    'objective': 'binary',\n    'device': 'gpu',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    \"num_leaves\": 77,\n    \"max_depth\": 15,\n    \"min_data_in_leaf\": 23,\n    \"min_gain_to_split\": 0.17931836655003727,\n    \"learning_rate\": 0.019740808893122665,\n    \"feature_fraction\": 0.7499258780711098,\n    \"bagging_fraction\": 0.9392312065171743,\n    \"bagging_freq\": 3,\n    \"lambda_l1\": 0.13817541814163015,\n    \"lambda_l2\": 5.987592011786754,\n    \"min_sum_hessian_in_leaf\": 2.7742348039686524,\n    'verbosity': -1\n}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Fold分割し格納\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X,y)):\n    print(\"#\"*25)\n    print(f\"### Fold {fold+1}\")\n    print(\"#\"*25)\n\n    X_train = X.iloc[train_idx,:].copy()\n    y_train = y.iloc[train_idx].copy()\n    X_valid = X.iloc[valid_idx,:].copy()\n    y_valid = y.iloc[valid_idx].copy()\n    X_test = test_.copy()\n\n    # ターゲットエンコーディング\n    CC = CATS1+CATS2\n    print(f\"Target encoding {len(CC)} features... \",end=\"\")\n    \n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n            \n        TE0 = TargetEncoder(n_folds=5, smooth=10, split_method='random', stat='mean')\n        X_train[c] = TE0.fit_transform(X_train[c],y_train).astype('float32')\n        X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        X_test[c] = TE0.transform(X_test[c]).astype('float32')\n        \n    print()\n\n    # TE0を明示的に削除\n    del TE0\n    gc.collect()\n\n    # CC以外はカテゴリ型に変換\n    for c in CATS:\n        if c not in CC:  \n            X_train[c] = X_train[c].astype('category')\n            X_valid[c] = X_valid[c].astype('category')\n            X_test[c]  = X_test[c].astype('category')    \n\n    # データセット\n    lgb_train = lgb.Dataset(\n        X_train,y_train,categorical_feature=CATS)\n\n    lgb_valid = lgb.Dataset(\n        X_valid,y_valid,categorical_feature=CATS)\n    # --------------------------\n    # 学習\n    # --------------------------\n    model_lgb = lgb.train(\n        lgbm_params,\n        lgb_train,\n        num_boost_round=5000,\n        valid_sets=[lgb_train, lgb_valid],\n        valid_names=[\"train\", \"valid\"],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100, verbose=False),\n            lgb.log_evaluation(100),\n        ]\n    )\n\n    # 予測\n    pred_lgb1[valid_idx] = model_lgb.predict(\n        X_valid, num_iteration=model_lgb.best_iteration)\n\n    pred_lgb_test1 += model_lgb.predict(\n        X_test, num_iteration=model_lgb.best_iteration)/5\n\n    # モデル保存\n    models_lgb1.append(model_lgb)\n\n    # メモリ開放\n    del X_train, X_valid\n    gc.collect()\n\n    # メモリ開放\n    import numba.cuda as cuda\n    cuda.current_context().deallocations.clear()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:34:25.615405Z","iopub.execute_input":"2025-09-15T01:34:25.616090Z","iopub.status.idle":"2025-09-15T01:37:12.867796Z","shell.execute_reply.started":"2025-09-15T01:34:25.616056Z","shell.execute_reply":"2025-09-15T01:37:12.867162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 訓練データのスコア\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\nAUC_lgb1 = roc_auc_score(y,pred_lgb1)\nprint(f\"LGB1: AUC score = {AUC_lgb1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:37:12.869103Z","iopub.execute_input":"2025-09-15T01:37:12.869380Z","iopub.status.idle":"2025-09-15T01:37:13.244506Z","shell.execute_reply.started":"2025-09-15T01:37:12.869355Z","shell.execute_reply":"2025-09-15T01:37:13.243668Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【② Light GBM (外部データセットのTE追加)】","metadata":{}},{"cell_type":"code","source":"############################################################\n############ Light GBM with OOF Target Encoding ############\n############################################################\n# 学習、バリデーションデータ\npred_lgb2 = np.zeros(len(train2))\npred_lgb_test2 = np.zeros(len(test2))\nmodels_lgb2 = []\n\n# 入力データ\nX = train2.drop([\"id\",\"y\"], axis=1).copy()\ny = train2[\"y\"].copy()\ntest_ = test2.drop([\"id\",\"y\"], axis=1).copy()\n\n# LightGBMパラメータ\nlgbm_params = {\n    'objective': 'binary',\n    'device': 'gpu',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    \"num_leaves\": 77,\n    \"max_depth\": 15,\n    \"min_data_in_leaf\": 23,\n    \"min_gain_to_split\": 0.17931836655003727,\n    \"learning_rate\": 0.019740808893122665,\n    \"feature_fraction\": 0.7499258780711098,\n    \"bagging_fraction\": 0.9392312065171743,\n    \"bagging_freq\": 3,\n    \"lambda_l1\": 0.13817541814163015,\n    \"lambda_l2\": 5.987592011786754,\n    \"min_sum_hessian_in_leaf\": 2.7742348039686524,\n    'verbosity': -1\n}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Fold分割し格納\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(X,y)):\n    print(\"#\"*25)\n    print(f\"### Fold {fold+1}\")\n    print(\"#\"*25)\n\n    X_train = X.iloc[train_idx,:].copy()\n    y_train = y.iloc[train_idx].copy()\n    X_valid = X.iloc[valid_idx,:].copy()\n    y_valid = y.iloc[valid_idx].copy()\n    X_test = test_.copy()\n\n    # ターゲットエンコーディング\n    CC = CATS1+CATS2\n    print(f\"Target encoding {len(CC)} features... \",end=\"\")\n\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n            \n        TE0 = TargetEncoder(n_folds=5, smooth=10, split_method='random', stat='mean')\n        X_train[c] = TE0.fit_transform(X_train[c],y_train).astype('float32')\n        X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        X_test[c] = TE0.transform(X_test[c]).astype('float32')\n        \n    print()\n\n    # TE0を明示的に削除\n    del TE0\n    gc.collect()\n\n    # CC以外はカテゴリ型に変換\n    for c in CATS:\n        if c not in CC:  \n            X_train[c] = X_train[c].astype('category')\n            X_valid[c] = X_valid[c].astype('category')\n            X_test[c]  = X_test[c].astype('category')    \n\n    # データセット\n    lgb_train = lgb.Dataset(\n        X_train,y_train,categorical_feature=CATS)\n\n    lgb_valid = lgb.Dataset(\n        X_valid,y_valid,categorical_feature=CATS)\n    # --------------------------\n    # 学習\n    # --------------------------\n    model_lgb = lgb.train(\n        lgbm_params,\n        lgb_train,\n        num_boost_round=5000,\n        valid_sets=[lgb_train, lgb_valid],\n        valid_names=[\"train\", \"valid\"],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100, verbose=False),\n            lgb.log_evaluation(100),\n        ]\n    )\n\n    # 予測\n    pred_lgb2[valid_idx] = model_lgb.predict(\n        X_valid, num_iteration=model_lgb.best_iteration)\n\n    pred_lgb_test2 += model_lgb.predict(\n        X_test, num_iteration=model_lgb.best_iteration)/5\n\n    # モデル保存\n    models_lgb2.append(model_lgb)\n\n    # メモリ開放\n    del X_train, X_valid\n    gc.collect()\n\n    # メモリ開放\n    import numba.cuda as cuda\n    cuda.current_context().deallocations.clear()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:37:13.245540Z","iopub.execute_input":"2025-09-15T01:37:13.245813Z","iopub.status.idle":"2025-09-15T01:40:32.297118Z","shell.execute_reply.started":"2025-09-15T01:37:13.245783Z","shell.execute_reply":"2025-09-15T01:40:32.296522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 訓練データのスコア\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\n\nAUC_lgb2 = roc_auc_score(y,pred_lgb2)\nprint(f\"LGB2: AUC score = {AUC_lgb2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:40:32.299152Z","iopub.execute_input":"2025-09-15T01:40:32.299383Z","iopub.status.idle":"2025-09-15T01:40:32.592242Z","shell.execute_reply.started":"2025-09-15T01:40:32.299366Z","shell.execute_reply":"2025-09-15T01:40:32.591368Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### XGBoost + Optuna","metadata":{}},{"cell_type":"code","source":"# ###########################################\n# ############ Light GBM + Optuna ###########\n# ###########################################\n# import optuna\n# import numpy as np\n# import pandas as pd\n# import xgboost as xgb\n# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import StratifiedKFold, KFold\n# # from category_encoders import TargetEncoder # CPU版\n# from cuml.preprocessing import TargetEncoder\n# import cudf\n# import warnings\n# import gc\n# warnings.filterwarnings(\"ignore\")\n\n# # 入力データ\n# X = train1.drop([\"id\",\"y\"], axis=1).copy()\n# y = train1[\"y\"].copy()\n\n# # 評価履歴を保存する辞書\n# evals_result_xgb = {}\n\n# def objective(trial):\n\n#     # 整数は±50%、確率系は±0.1〜0.2、正則化は対数で±1〜2\n#     # XGBoostパラメータ\n#     xgb_params = {\n#         \"objective\": \"binary:logistic\",\n#         \"eval_metric\": \"auc\",\n#         \"tree_method\": \"gpu_hist\",\n#         \"device\": \"cuda\",\n#         \"grow_policy\": \"lossguide\",\n\n#         \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n#         \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 10.0, log=True),\n\n#         \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n#         \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n\n#         \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n#         \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n    \n#         \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n#         \"max_leaves\": trial.suggest_int(\"max_leaves\", 16, 256),\n#     }\n\n#     pred_xgb = np.zeros(len(X))\n#     fold_scores = []\n\n#     skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n#     # kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n#     # Fold分割し格納\n#     for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n#         print(\"#\"*25)\n#         print(f\"### Fold {fold+1}\")\n#         print(\"#\"*25)\n\n#         X_train = X.iloc[train_idx,:].copy()\n#         y_train = y.iloc[train_idx].copy()\n#         X_valid = X.iloc[valid_idx,:].copy()\n#         y_valid = y.iloc[valid_idx].copy()\n\n#         # ターゲットエンコーディング\n#         CC = CATS1_in+CATS2_in\n#         print(f\"Target encoding {len(CC)} features... \",end=\"\")\n\n#         for i,c in enumerate(CC):\n#             if i%10==0: print(f\"{i}, \",end=\"\")\n            \n#             TE0 = TargetEncoder(n_folds=5, smooth=10, \n#                                 split_method='random', stat='mean')\n#             X_train[c] = TE0.fit_transform(X_train[c],y_train).astype('float32')\n#             X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        \n#         print()\n\n#         # TE0を明示的に削除\n#         del TE0\n#         gc.collect()\n\n#         # CC以外はカテゴリ型に変換\n#         for c in CATS_in:\n#             if c not in CC:  \n#                 X_train[c] = X_train[c].astype('category')\n#                 X_valid[c] = X_valid[c].astype('category')\n\n#         # DMatrixに変換\n#         dtrain = xgb.DMatrix(X_train,label=y_train,enable_categorical=True)\n#         dvalid = xgb.DMatrix(X_valid,label=y_valid,enable_categorical=True)\n\n#         # 学習\n#         model_xgb = xgb.train(\n#             xgb_params,\n#             dtrain,\n#             num_boost_round=2000,\n#             evals=[(dtrain,\"train\"),(dvalid,\"valid\")],\n#             early_stopping_rounds=100,\n#             evals_result=evals_result_xgb,\n#             verbose_eval=500,\n#         )\n\n#         # 各foldでのバリデーション予測\n#         pred_xgb[valid_idx] = model_xgb.predict(\n#             dvalid, iteration_range=(0,model_xgb.best_iteration+1))\n\n#         # AUCスコア算出\n#         fold_scores.append(roc_auc_score(y_valid,pred_xgb[valid_idx]))\n\n#     # 各Foldの平均値\n#     score = np.mean(fold_scores)\n    \n#     return score\n\n# # 最適化実行\n# study = optuna.create_study(direction=\"maximize\",\n#                            pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10))\n# study.optimize(objective, n_trials=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:40:32.593327Z","iopub.execute_input":"2025-09-15T01:40:32.593584Z","iopub.status.idle":"2025-09-15T01:40:32.599715Z","shell.execute_reply.started":"2025-09-15T01:40:32.593556Z","shell.execute_reply":"2025-09-15T01:40:32.599079Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【③ XGBoost (内部データセットのみ)】","metadata":{}},{"cell_type":"code","source":"#################################################\n############ XGBoost ############################\n#################################################\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\n# 学習、バリデーションデータ\npred_xgb1 = np.zeros(len(train1))\npred_xgb_test1 = np.zeros(len(test1))\nmodels_xgb1 = []\n\n# 入力データ\nX = train1.drop([\"id\",\"y\"],axis=1).copy()\ny = train1[\"y\"].copy()\ntest_ = test1.drop([\"id\",\"y\"],axis=1).copy()\n\n# 評価履歴を保存する辞書\nevals_result_xgb = {}\n\n# パラメータ\nxgb_params = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"auc\",\n    \"tree_method\": \"gpu_hist\",\n    \"device\": \"cuda\",\n    \"grow_policy\": \"lossguide\",\n    \"learning_rate\": 0.020187734867721113,\n    \"max_depth\": 10,\n    \"min_child_weight\": 0.0015137166209180514,\n    \"subsample\": 0.6786153011677415,\n    \"colsample_bytree\": 0.7917555828184474,\n    \"colsample_bylevel\": 0.5539530181906183,\n    \"reg_alpha\": 5.3805307261170965,\n    \"reg_lambda\": 1.2434258141601598e-08,\n    \"gamma\": 3.715076866606369,\n    \"max_leaves\": 91,\n}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold,(train_idx,valid_idx) in enumerate(skf.split(X,y)):\n\n    print(\"#\"*25)\n    print(f\"### Fold {fold+1}\")\n    print(\"#\"*25)\n\n    # foldごとの訓練、バリデーションデータ\n    X_train = X.iloc[train_idx,:].copy()\n    y_train = y.iloc[train_idx].copy()\n    X_valid = X.iloc[valid_idx,:].copy()\n    y_valid = y.iloc[valid_idx].copy()\n    X_test = test_.copy()\n\n    # ターゲットエンコーディング\n    CC = CATS1+CATS2\n    print(f\"Target encoding {len(CC)} features... \",end=\"\")\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n        TE0 = TargetEncoder(n_folds=5, smooth=0, split_method='random', stat='mean')\n        X_train[c] = TE0.fit_transform(X_train[c],ｙ_train).astype('float32')\n        X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        X_test[c] = TE0.transform(X_test[c]).astype('float32')\n    print()\n\n    # CC以外はカテゴリ型に変換\n    for c in CATS:\n        if c not in CC:  \n            X_train[c] = X_train[c].astype('category')\n            X_valid[c] = X_valid[c].astype('category')\n\n    # DMatrixに変換\n    dtrain = xgb.DMatrix(X_train,label=y_train,enable_categorical=True)\n    dvalid = xgb.DMatrix(X_valid,label=y_valid,enable_categorical=True)\n    dtest = xgb.DMatrix(X_test,enable_categorical=True)\n\n    # 学習\n    model_xgb = xgb.train(\n        xgb_params,\n        dtrain,\n        num_boost_round=5000,\n        evals=[(dtrain,\"train\"),(dvalid,\"valid\")],\n        early_stopping_rounds=100,\n        evals_result=evals_result_xgb,\n        verbose_eval=100,\n    )\n\n    # 各foldでのバリデーション予測\n    pred_xgb1[valid_idx] = model_xgb.predict(\n        dvalid, iteration_range=(0,model_xgb.best_iteration+1))\n\n    # 各foldでのバリデーション予測\n    pred_xgb_test1 += model_xgb.predict(\n        dtest, iteration_range=(0,model_xgb.best_iteration+1))/5\n    \n    # モデルの追加\n    models_xgb1.append(model_xgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:40:32.600556Z","iopub.execute_input":"2025-09-15T01:40:32.600810Z","iopub.status.idle":"2025-09-15T01:42:45.548817Z","shell.execute_reply.started":"2025-09-15T01:40:32.600786Z","shell.execute_reply":"2025-09-15T01:42:45.548025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 訓練データのスコア\nfrom sklearn.metrics import f1_score\n\nAUC_xgb1 = roc_auc_score(y,pred_xgb1)\nprint(f\"XGB1: AUC score = {AUC_xgb1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:42:45.549678Z","iopub.execute_input":"2025-09-15T01:42:45.549944Z","iopub.status.idle":"2025-09-15T01:42:45.849138Z","shell.execute_reply.started":"2025-09-15T01:42:45.549921Z","shell.execute_reply":"2025-09-15T01:42:45.848471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【④ XGBoost (外部データセットのTE追加)】","metadata":{}},{"cell_type":"code","source":"#################################################\n############ XGBoost ############################\n#################################################\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\n# 学習、バリデーションデータ\npred_xgb2 = np.zeros(len(train2))\npred_xgb_test2 = np.zeros(len(test2))\nmodels_xgb2 = []\n\n# 入力データ\nX = train2.drop([\"id\",\"y\"],axis=1).copy()\ny = train2[\"y\"].copy()\ntest_ = test2.drop([\"id\",\"y\"],axis=1).copy()\n\n# 評価履歴を保存する辞書\nevals_result_xgb = {}\n\n# パラメータ\nxgb_params = {\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": \"auc\",\n    \"tree_method\": \"gpu_hist\",\n    \"device\": \"cuda\",\n    \"grow_policy\": \"lossguide\",\n    \"learning_rate\": 0.020187734867721113,\n    \"max_depth\": 10,\n    \"min_child_weight\": 0.0015137166209180514,\n    \"subsample\": 0.6786153011677415,\n    \"colsample_bytree\": 0.7917555828184474,\n    \"colsample_bylevel\": 0.5539530181906183,\n    \"reg_alpha\": 5.3805307261170965,\n    \"reg_lambda\": 1.2434258141601598e-08,\n    \"gamma\": 3.715076866606369,\n    \"max_leaves\": 91,\n}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold,(train_idx,valid_idx) in enumerate(skf.split(X,y)):\n\n    print(\"#\"*25)\n    print(f\"### Fold {fold+1}\")\n    print(\"#\"*25)\n\n    # foldごとの訓練、バリデーションデータ\n    X_train = X.iloc[train_idx,:].copy()\n    y_train = y.iloc[train_idx].copy()\n    X_valid = X.iloc[valid_idx,:].copy()\n    y_valid = y.iloc[valid_idx].copy()\n    X_test = test_.copy()\n\n    # ターゲットエンコーディング\n    CC = CATS1+CATS2\n    print(f\"Target encoding {len(CC)} features... \",end=\"\")\n    for i,c in enumerate(CC):\n        if i%10==0: print(f\"{i}, \",end=\"\")\n        TE0 = TargetEncoder(n_folds=5, smooth=0, split_method='random', stat='mean')\n        X_train[c] = TE0.fit_transform(X_train[c],ｙ_train).astype('float32')\n        X_valid[c] = TE0.transform(X_valid[c]).astype('float32')\n        X_test[c] = TE0.transform(X_test[c]).astype('float32')\n    print()\n\n    # CC以外はカテゴリ型に変換\n    for c in CATS:\n        if c not in CC:  \n            X_train[c] = X_train[c].astype('category')\n            X_valid[c] = X_valid[c].astype('category')\n\n    # DMatrixに変換\n    dtrain = xgb.DMatrix(X_train,label=y_train,enable_categorical=True)\n    dvalid = xgb.DMatrix(X_valid,label=y_valid,enable_categorical=True)\n    dtest = xgb.DMatrix(X_test,enable_categorical=True)\n\n    # 学習\n    model_xgb = xgb.train(\n        xgb_params,\n        dtrain,\n        num_boost_round=5000,\n        evals=[(dtrain,\"train\"),(dvalid,\"valid\")],\n        early_stopping_rounds=100,\n        evals_result=evals_result_xgb,\n        verbose_eval=100,\n    )\n\n    # 各foldでのバリデーション予測\n    pred_xgb2[valid_idx] = model_xgb.predict(\n        dvalid, iteration_range=(0,model_xgb.best_iteration+1))\n\n    # 各foldでのバリデーション予測\n    pred_xgb_test2 += model_xgb.predict(\n        dtest, iteration_range=(0,model_xgb.best_iteration+1))/5\n    \n    # モデルの追加\n    models_xgb2.append(model_xgb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:42:45.849976Z","iopub.execute_input":"2025-09-15T01:42:45.850269Z","iopub.status.idle":"2025-09-15T01:45:32.494615Z","shell.execute_reply.started":"2025-09-15T01:42:45.850245Z","shell.execute_reply":"2025-09-15T01:45:32.493999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 訓練データのスコア\nfrom sklearn.metrics import f1_score\n\nAUC_xgb2 = roc_auc_score(y,pred_xgb2)\nprint(f\"XGB2: AUC score = {AUC_xgb2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:45:32.495396Z","iopub.execute_input":"2025-09-15T01:45:32.495654Z","iopub.status.idle":"2025-09-15T01:45:32.791186Z","shell.execute_reply.started":"2025-09-15T01:45:32.495630Z","shell.execute_reply":"2025-09-15T01:45:32.790542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 【CatBoost】","metadata":{}},{"cell_type":"code","source":"# #################################################\n# ############ CatBoost (Classifier版) ############\n# #################################################\n# from catboost import CatBoostClassifier, Pool\n# from sklearn.model_selection import StratifiedKFold\n# import numpy as np\n\n# # 学習、バリデーションデータ\n# pred_cb = np.zeros(len(train_df))\n# pred_cb_test = np.zeros(len(test_df))\n# models_cb = []\n# cb_auc_valid = []  # foldごとのAUC履歴\n\n# # 入力データ\n# X = train.drop([\"id\",\"y\"],axis=1)\n# y = train[\"y\"]\n# X_test = test.drop([\"id\",\"y\"],axis=1)\n\n# # X = X_train_enc\n# # y = y_train\n# # X_test = X_test_enc\n\n# # CatBoostパラメータ\n# cat_params = {\n#     \"loss_function\": \"Logloss\",\n#     \"eval_metric\": \"AUC\",\n#     \"depth\": 8,                   # 6〜10\n#     \"learning_rate\": 0.05,        # 0.03〜0.1\n#     # \"iterations\": 1,          # 大きめ＋ES\n#     \"iterations\": 2000,          # 大きめ＋ES\n#     \"bootstrap_type\": \"Bayesian\", # 精度安定\n#     \"boosting_type\": \"Ordered\",   # 多カテゴリに強い\n#     \"random_strength\": 1.0,       # 0.5〜2.0で微調整\n#     \"task_type\": \"GPU\",\n#     # \"task_type\": \"CPU\",           # このデータ規模ならCPUの方が速い/安定なこと多い\n#     \"verbose\": 100,\n# }\n\n# # Stratified KFold\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n\n#     print(\"#\" * 25)\n#     print(f\"### Fold {fold+1}\")\n#     print(\"#\" * 25)\n\n#     # データ分割\n#     X_train_kf = X.iloc[train_idx, :]\n#     y_train_kf = y.iloc[train_idx]\n#     X_valid_kf = X.iloc[valid_idx, :]\n#     y_valid_kf = y.iloc[valid_idx]\n\n#     # object型をカテゴリ型に変換\n#     for col in cat_col:\n#         X_train_kf.loc[:, col] = X_train_kf.loc[:, col].astype(\"category\")\n#         X_valid_kf.loc[:, col] = X_valid_kf.loc[:, col].astype(\"category\")\n\n#     # Poolを作成\n#     train_pool = Pool(X_train_kf, y_train_kf, cat_features=cat_col)\n#     valid_pool = Pool(X_valid_kf, y_valid_kf, cat_features=cat_col)\n\n#     # モデル作成 & 学習\n#     model_cb = CatBoostClassifier(**cat_params)\n#     model_cb.fit(\n#         train_pool,\n#         eval_set=valid_pool,\n#         early_stopping_rounds=100,\n#         use_best_model=True\n#     )\n\n#     # バリデーション予測\n#     pred_cb[valid_idx] = model_cb.predict_proba(X_valid_kf)[:, 1]\n\n#     # モデル保存\n#     models_cb.append(model_cb)\n\n#     # foldごとのベストスコアを保存\n#     cb_auc_valid.append(model_cb.get_best_score()[\"validation\"][\"AUC\"])\n\n# # テスト予測\n# for model in models_cb:\n#     pred_cb_test += model.predict_proba(X_test)[:, 1]\n\n# # FOLD数で割って平均化\n# pred_cb_test = pred_cb_test / skf.n_splits\n\n# print(\"各foldのAUC:\", cb_auc_valid)\n# print(\"平均AUC:\", np.mean(cb_auc_valid))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:45:32.793062Z","iopub.execute_input":"2025-09-15T01:45:32.793339Z","iopub.status.idle":"2025-09-15T01:45:32.798046Z","shell.execute_reply.started":"2025-09-15T01:45:32.793321Z","shell.execute_reply":"2025-09-15T01:45:32.797361Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.metrics import f1_score\n\n# # スコア表示\n# AUC_cb = roc_auc_score(y,pred_cb)\n# F1_cb = f1_score(y,np.round(pred_cb,0))\n# print(f\"CB: AUC score = {AUC_cb}, F1 = {F1_cb}\")\n\n# # # 学習履歴を一番短いfoldに揃える\n# # min_len = min(len(m) for m in cb_auc_valid)\n# # cb_auc_score = [m[:min_len] for m in cb_auc_valid]\n\n# # # foldごとの結果を平均する\n# # cb_auc_score = np.average(cb_auc_score,axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:45:32.798757Z","iopub.execute_input":"2025-09-15T01:45:32.799002Z","iopub.status.idle":"2025-09-15T01:45:32.817676Z","shell.execute_reply.started":"2025-09-15T01:45:32.798980Z","shell.execute_reply":"2025-09-15T01:45:32.817083Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 提出データ作成\n# sample_submission = pd.read_csv(\"/kaggle/input/playground-series-s5e8/sample_submission.csv\")\n\n# sample_submission['y'] = pred_cb_test\n# sample_submission.to_csv('submission.csv', index=False)\n# print('Submission file saved.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:45:32.818384Z","iopub.execute_input":"2025-09-15T01:45:32.818614Z","iopub.status.idle":"2025-09-15T01:45:32.839319Z","shell.execute_reply.started":"2025-09-15T01:45:32.818590Z","shell.execute_reply":"2025-09-15T01:45:32.838745Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import  matplotlib.pyplot as plt\n# # 履歴の可視化\n# plt.plot(cb_auc_score, label='Validation')\n# plt.xlabel('Iteration')\n# plt.ylabel('AUC')\n# plt.grid()\n# plt.legend()\n# plt.title(\"CabBoost AUC\")\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:45:32.839929Z","iopub.execute_input":"2025-09-15T01:45:32.840086Z","iopub.status.idle":"2025-09-15T01:45:32.855742Z","shell.execute_reply.started":"2025-09-15T01:45:32.840074Z","shell.execute_reply":"2025-09-15T01:45:32.855242Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stacking①③","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import StackingClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import cross_val_score, StratifiedKFold\n# from sklearn.metrics import roc_auc_score\n\n# print(\"# STACKING ENSEMBLE IMPLEMENTATION\")\n# print(\"# \" + \"=\"*50)\n# print(\"# Combining top 2 models: LightGBM①, XGBoost①\")\n# print(\"# Meta-learner: Logistic Regression\")\n# print(\"# \" + \"=\"*50)\n\n# y = train1[\"y\"].copy()\n\n# stacking_train = pd.DataFrame({\n#     'lgb1': pred_lgb1,\n#     'xgb1': pred_xgb1, \n# })\n\n# stacking_test = pd.DataFrame({\n#     'lgb1': pred_lgb_test1,\n#     'xgb1': pred_xgb_test1,\n# })\n\n# print(f\"# Stacking train shape: {stacking_train.shape}\")\n# print(f\"# Stacking test shape: {stacking_test.shape}\")\n\n# print(\"\\n# METHOD 1: WEIGHTED AVERAGE\")\n# print(\"# \" + \"-\"*30)\n\n# scores = [AUC_lgb1, AUC_xgb1]  \n# total_score = sum(scores)\n# weights = [score/total_score for score in scores]\n\n# print(f\"# Model weights:\")\n# print(f\"# LightGBM1: {weights[0]:.4f}\")\n# print(f\"# XGBoost1:  {weights[1]:.4f}\")\n\n# weighted_oof = (\n#     stacking_train['lgb1'] * weights[0] + \n#     stacking_train['xgb1'] * weights[1] \n# )\n\n# weighted_test = (\n#     stacking_test['lgb1'] * weights[0] + \n#     stacking_test['xgb1'] * weights[1] \n# ) \n\n# weighted_score = roc_auc_score(y, weighted_oof)\n# print(f\"# Weighted Average ROC AUC: {weighted_score:.6f}\")\n\n# print(\"\\n# METHOD 2: LOGISTIC REGRESSION META-LEARNER\")\n# print(\"# \" + \"-\"*40)\n\n# meta_learner = LogisticRegression(random_state=42, max_iter=1000)\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# cv_scores = cross_val_score(meta_learner, stacking_train, y, \n#                            cv=skf, scoring='roc_auc', n_jobs=1)\n\n# print(f\"# Meta-learner CV scores: {[f'{score:.6f}' for score in cv_scores]}\")\n# print(f\"# Meta-learner mean CV: {cv_scores.mean():.6f} ± {cv_scores.std():.6f}\")\n\n# meta_learner.fit(stacking_train, y)\n# meta_oof = meta_learner.predict_proba(stacking_train)[:, 1]\n# meta_test = meta_learner.predict_proba(stacking_test)[:, 1]\n# meta_score = roc_auc_score(y, meta_oof)\n\n# print(f\"# Meta-learner ROC AUC: {meta_score:.6f}\")\n\n# coefficients = meta_learner.coef_[0]\n# print(f\"# Meta-learner coefficients:\")\n# print(f\"# LightGBM1: {coefficients[0]:.4f}\")\n# print(f\"# XGBoost1:  {coefficients[1]:.4f}\")\n# print(f\"# Intercept: {meta_learner.intercept_[0]:.4f}\")\n\n# print(\"\\n# METHOD 3: SIMPLE AVERAGE (BASELINE)\")\n# print(\"# \" + \"-\"*35)\n\n# simple_oof = (\n#     stacking_train['lgb1'] + \n#     stacking_train['xgb1'] \n#     ) / 2\n# simple_test = (\n#     stacking_test['lgb1'] + \n#     stacking_test['xgb1'] \n#     ) / 2\n# simple_score = roc_auc_score(y, simple_oof)\n\n# print(f\"# Simple Average ROC AUC: {simple_score:.6f}\")\n\n# print(\"\\n# ENSEMBLE METHODS COMPARISON\")\n# print(\"# \" + \"=\"*40)\n# ensemble_results = [\n#     ('Individual LightGBM1', AUC_lgb1),\n#     ('Individual XGBoost1', AUC_xgb1),\n#     ('Weighted Average', weighted_score),\n#     ('Meta-learner (LogReg)', meta_score),\n#     ('Simple Average', simple_score)\n# ]\n\n# ensemble_results.sort(key=lambda x: x[1], reverse=True)\n\n# for i, (method, score) in enumerate(ensemble_results, 1):\n#     print(f\"# {i}. {method:<25}: {score:.6f}\")\n\n# best_method, best_score = ensemble_results[0]\n# print(f\"\\n# BEST ENSEMBLE METHOD: {best_method}\")\n# print(f\"# BEST ENSEMBLE SCORE: {best_score:.6f}\")\n\n# if 'Meta-learner' in best_method:\n#     final_oof1 = meta_oof\n#     final_test1 = meta_test\n#     print(\"# Using Meta-learner predictions for final submission\")\n# elif 'Weighted' in best_method:\n#     final_oof1 = weighted_oof\n#     final_test1 = weighted_test\n#     print(\"# Using Weighted Average predictions for final submission\")\n# else:\n#     final_oof1 = simple_oof\n#     final_test1 = simple_test\n#     print(\"# Using Simple Average predictions for final submission\")\n\n# print(\"\\n# STACKING ENSEMBLE COMPLETED!\")\n# print(\"# \" + \"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:45:32.856419Z","iopub.execute_input":"2025-09-15T01:45:32.856635Z","iopub.status.idle":"2025-09-15T01:45:44.184606Z","shell.execute_reply.started":"2025-09-15T01:45:32.856613Z","shell.execute_reply":"2025-09-15T01:45:44.183649Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stacking②④","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import StackingClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import cross_val_score, StratifiedKFold\n# from sklearn.metrics import roc_auc_score\n\n# print(\"# STACKING ENSEMBLE IMPLEMENTATION\")\n# print(\"# \" + \"=\"*50)\n# print(\"# Combining top 2 models: LightGBM②, XGBoost②\")\n# print(\"# Meta-learner: Logistic Regression\")\n# print(\"# \" + \"=\"*50)\n\n# y = train2[\"y\"].copy()\n\n# stacking_train = pd.DataFrame({\n#     'lgb2': pred_lgb2,\n#     'xgb2': pred_xgb2, \n# })\n\n# stacking_test = pd.DataFrame({\n#     'lgb2': pred_lgb_test2,\n#     'xgb2': pred_xgb_test2,\n# })\n\n# print(f\"# Stacking train shape: {stacking_train.shape}\")\n# print(f\"# Stacking test shape: {stacking_test.shape}\")\n\n# print(\"\\n# METHOD 1: WEIGHTED AVERAGE\")\n# print(\"# \" + \"-\"*30)\n\n# scores = [AUC_lgb2, AUC_xgb2]  \n# total_score = sum(scores)\n# weights = [score/total_score for score in scores]\n\n# print(f\"# Model weights:\")\n# print(f\"# LightGBM2: {weights[0]:.4f}\")\n# print(f\"# XGBoost2:  {weights[1]:.4f}\")\n\n# weighted_oof = (\n#     stacking_train['lgb2'] * weights[0] + \n#     stacking_train['xgb2'] * weights[1] \n# )\n\n# weighted_test = (\n#     stacking_test['lgb2'] * weights[0] + \n#     stacking_test['xgb2'] * weights[1] \n# ) \n\n# weighted_score = roc_auc_score(y, weighted_oof)\n# print(f\"# Weighted Average ROC AUC: {weighted_score:.6f}\")\n\n# print(\"\\n# METHOD 2: LOGISTIC REGRESSION META-LEARNER\")\n# print(\"# \" + \"-\"*40)\n\n# meta_learner = LogisticRegression(random_state=42, max_iter=1000)\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# cv_scores = cross_val_score(meta_learner, stacking_train, y, \n#                            cv=skf, scoring='roc_auc', n_jobs=1)\n\n# print(f\"# Meta-learner CV scores: {[f'{score:.6f}' for score in cv_scores]}\")\n# print(f\"# Meta-learner mean CV: {cv_scores.mean():.6f} ± {cv_scores.std():.6f}\")\n\n# meta_learner.fit(stacking_train, y)\n# meta_oof = meta_learner.predict_proba(stacking_train)[:, 1]\n# meta_test = meta_learner.predict_proba(stacking_test)[:, 1]\n# meta_score = roc_auc_score(y, meta_oof)\n\n# print(f\"# Meta-learner ROC AUC: {meta_score:.6f}\")\n\n# coefficients = meta_learner.coef_[0]\n# print(f\"# Meta-learner coefficients:\")\n# print(f\"# LightGBM2: {coefficients[0]:.4f}\")\n# print(f\"# XGBoost2:  {coefficients[1]:.4f}\")\n# print(f\"# Intercept: {meta_learner.intercept_[0]:.4f}\")\n\n# print(\"\\n# METHOD 3: SIMPLE AVERAGE (BASELINE)\")\n# print(\"# \" + \"-\"*35)\n\n# simple_oof = (\n#     stacking_train['lgb2'] + \n#     stacking_train['xgb2'] \n#     ) / 2\n# simple_test = (\n#     stacking_test['lgb2'] + \n#     stacking_test['xgb2'] \n#     ) / 2\n# simple_score = roc_auc_score(y, simple_oof)\n\n# print(f\"# Simple Average ROC AUC: {simple_score:.6f}\")\n\n# print(\"\\n# ENSEMBLE METHODS COMPARISON\")\n# print(\"# \" + \"=\"*40)\n# ensemble_results = [\n#     ('Individual LightGBM2', AUC_lgb2),\n#     ('Individual XGBoost2', AUC_xgb2),\n#     ('Weighted Average', weighted_score),\n#     ('Meta-learner (LogReg)', meta_score),\n#     ('Simple Average', simple_score)\n# ]\n\n# ensemble_results.sort(key=lambda x: x[1], reverse=True)\n\n# for i, (method, score) in enumerate(ensemble_results, 1):\n#     print(f\"# {i}. {method:<25}: {score:.6f}\")\n\n# best_method, best_score = ensemble_results[0]\n# print(f\"\\n# BEST ENSEMBLE METHOD: {best_method}\")\n# print(f\"# BEST ENSEMBLE SCORE: {best_score:.6f}\")\n\n# if 'Meta-learner' in best_method:\n#     final_oo2 = meta_oof\n#     final_test2 = meta_test\n#     print(\"# Using Meta-learner predictions for final submission\")\n# elif 'Weighted' in best_method:\n#     final_oo2 = weighted_oof\n#     final_test2 = weighted_test\n#     print(\"# Using Weighted Average predictions for final submission\")\n# else:\n#     final_oo2 = simple_oof\n#     final_test2 = simple_test\n#     print(\"# Using Simple Average predictions for final submission\")\n\n# print(\"\\n# STACKING ENSEMBLE COMPLETED!\")\n# print(\"# \" + \"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:46:40.120956Z","iopub.execute_input":"2025-09-15T01:46:40.121510Z","iopub.status.idle":"2025-09-15T01:46:50.698013Z","shell.execute_reply.started":"2025-09-15T01:46:40.121483Z","shell.execute_reply":"2025-09-15T01:46:50.697136Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 提出データ作成\nsample_submission = pd.read_csv(\"/kaggle/input/playground-series-s5e8/sample_submission.csv\")\n\nsample_submission['y'] = (pred_lgb_test1 + pred_lgb_test2 + pred_xgb_test1 + pred_xgb_test2) / 4\nsample_submission.to_csv('submission.csv', index=False)\nprint('Submission file saved.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:52:23.532634Z","iopub.execute_input":"2025-09-15T01:52:23.533428Z","iopub.status.idle":"2025-09-15T01:52:23.564984Z","shell.execute_reply.started":"2025-09-15T01:52:23.533402Z","shell.execute_reply":"2025-09-15T01:52:23.564237Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)【Neural Net】","metadata":{}},{"cell_type":"code","source":"# import random\n# import os\n# import pandas as pd\n# import numpy as np\n# from tqdm.notebook import tqdm\n# import matplotlib.pyplot as plt\n\n# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n# from sklearn.metrics import f1_score\n\n# # pytorch実装\n# import torch # Tensorの作成や操作\n# import torch.nn as nn # ニューラルネットワーク\n# import torch.nn.functional as F # 関数をメソッドとして提供\n# import torch.optim as optim # オプティマイザ\n# from torch.utils.data import Dataset, DataLoader\n# from torch.autograd import Variable\n\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import roc_auc_score\n# from tqdm.notebook import tqdm\n# # from tqdm import tqdm\n# import matplotlib.pyplot as plt\n# import time\n\n# # GPUの使用状況確認\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# print(device)\n\n# all_df = all_df_NN\n# all_df = all_df.drop([\"id\",\"y\"],axis=1)\n# y = train_df[\"y\"]\n\n# # 設定\n# SEED = 42\n# TARGET = \"y\"\n\n# CATEGORICAL = cat_col\n# NUMERICAL = num_col\n# USE = CATEGORICAL + NUMERICAL\n# # df_train = train_df.drop(\"id\",axis=1)\n# # df_test = test_df.drop(\"id\",axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.378905Z","iopub.status.idle":"2025-09-15T01:28:35.379252Z","shell.execute_reply.started":"2025-09-15T01:28:35.379059Z","shell.execute_reply":"2025-09-15T01:28:35.379073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # (1) 住宅ローン + ローン\n# all_df[\"housing_loan\"] = all_df[\"housing\"].astype(str) + \"_\" + all_df[\"loan\"].astype(str)\n\n# # (2) コンタクト時間 x 年齢\n# all_df[\"duration_x_age\"] = all_df[\"duration\"] * all_df[\"age\"]\n\n# # (3) sin,cos(コンタクト時間)\n# all_df['duration_sin'] = np.sin(2*np.pi * all_df['duration'] / 400)\n# all_df['duration_cos'] = np.cos(2*np.pi * all_df['duration'] / 400)\n\n# # (4) monthを数値に直し周期的に使う\n# month_map = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,\n#     'may': 5, 'jun': 6, 'jul': 7, 'aug': 8,\n#     'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n# all_df['month_num'] = all_df['month'].map(month_map).astype('int')\n# all_df['month_sin'] = np.sin(2 * np.pi * all_df['month_num'] / 12)\n# all_df['month_cos'] = np.cos(2 * np.pi * all_df['month_num'] / 12)\n\n# # (5) コンタクト時間をカテゴリ化\n# all_df['duration_bin'] = pd.cut(\n#     all_df['duration'],\n#     bins=[0, 60, 300, 600, 900, float('inf')],\n#     labels=['short', 'medium', 'long', 'very_long', 'extreme'],\n#     right=False)\n# all_df['duration_bin'] = all_df['duration_bin'].astype(\"object\")\n\n# # (6) 連絡手段 + 年齢\n# all_df['age_group'] = pd.cut(\n#     all_df['age'],\n#     bins=[0, 30, 45, 60, 100],\n#     labels=['young', 'mid', 'senior', 'elder'])\n# all_df[\"contact_age\"] = all_df[\"contact\"].astype(str) + \"_\" + all_df[\"age_group\"].astype(str)\n# all_df = all_df.drop(\"age_group\",axis=1)\n\n# # (7) sin,cos(pdays)\n# all_df['pdays_sin'] = np.sin(2*np.pi * all_df['pdays'] / 90)\n# all_df['pdays_cos'] = np.cos(2*np.pi * all_df['pdays'] / 90)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.380036Z","iopub.status.idle":"2025-09-15T01:28:35.380369Z","shell.execute_reply.started":"2025-09-15T01:28:35.380199Z","shell.execute_reply":"2025-09-15T01:28:35.380230Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 標準化 + ラベルエンコード\n# def preprocessing(all_df, cat_cols=CATEGORICAL, num_cols=NUMERICAL, target=TARGET):\n\n#     # 訓練データとテストデータに分離\n#     train = all_df[:len(train_df)]\n#     test = all_df[len(train_df):]\n\n#     # y = train[target]\n#     # train = train.drop(\"y\",axis=1)\n#     train_len = len(train)\n\n#     # 訓練データ + テストデータ\n#     # df = pd.concat([train.drop(columns=target), test])\n#     # y = train[target]\n#     # train_len = len(train)\n    \n#     # 欠損埋め\n#     # df[cat_cols] = df[cat_cols].fillna('None')\n#     # df[num_cols] = df[num_cols].fillna(0)\n\n#     # train = df[:train_len]\n#     # test = df[train_len:]\n\n#     # 標準化\n#     scaler = StandardScaler()\n\n#     # フィッティング\n#     # scaler.fit(df[num_cols])\n#     scaler.fit(train[num_cols])\n\n#     # 適用\n#     train[num_cols] = scaler.transform(train[num_cols])\n#     test[num_cols] = scaler.transform(test[num_cols])\n#     df = pd.concat([train, test])\n    \n#     # ラベルエンコーダ\n#     for col in df.columns:\n#         if col in cat_cols:\n#             df[col] = LabelEncoder().fit_transform(df[col])\n#             df[col]= df[col].astype('category')\n            \n#     return pd.concat([df.iloc[:train_len], y], axis=1), df.iloc[train_len:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.381391Z","iopub.status.idle":"2025-09-15T01:28:35.381637Z","shell.execute_reply.started":"2025-09-15T01:28:35.381522Z","shell.execute_reply":"2025-09-15T01:28:35.381533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 前処理の実施\n# df_train, df_test = preprocessing(all_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.382362Z","iopub.status.idle":"2025-09-15T01:28:35.382610Z","shell.execute_reply.started":"2025-09-15T01:28:35.382499Z","shell.execute_reply":"2025-09-15T01:28:35.382510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # データセット関数\n# class CustomDataset(Dataset):\n\n#     # オブジェクト定義\n#     def __init__(self, df, target, cat_cols=CATEGORICAL):\n#         self.df_cat = df[cat_cols]\n#         self.df_num = df.drop(cat_cols, axis=1)\n#         self.X_cats = self.df_cat.values.astype(np.int64)\n#         self.X_nums = self.df_num.values.astype(np.float32)\n#         self.target = target.values.astype(np.int64)\n\n#     # データセットのサイズを返す\n#     def __len__(self):\n#         return len(self.target)\n\n#     # 指定したインデックスのデータとラベルを返す\n#     def __getitem__(self, idx):\n#         return [self.X_cats[idx], self.X_nums[idx], self.target[idx]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.383802Z","iopub.status.idle":"2025-09-15T01:28:35.384091Z","shell.execute_reply.started":"2025-09-15T01:28:35.383945Z","shell.execute_reply":"2025-09-15T01:28:35.383958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # NNモデル作成\n# class NN_Model(nn.Module):\n\n#     # ネットワーク構造の定義\n#     def __init__(self, embedding_sizes, n_num):\n#         super().__init__()\n#         self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories, size in embedding_sizes])\n#         n_emb = sum(e.embedding_dim for e in self.embeddings)\n#         self.n_emb, self.n_num = n_emb, n_num\n#         self.lin1 = nn.Linear(self.n_emb + self.n_num, 100)\n#         self.lin2 = nn.Linear(100, 70)\n#         self.lin3 = nn.Linear(70, 2)\n#         self.bn1 = nn.BatchNorm1d(self.n_num)\n#         self.bn2 = nn.BatchNorm1d(100)\n#         self.bn3 = nn.BatchNorm1d(70)\n#         self.emb_drop = nn.Dropout(0.6)\n#         self.drops = nn.Dropout(0.3)\n \n#     # 順伝播\n#     def forward(self,x_cat,x_num):\n#         x = [e(x_cat[:, i]) for i, e in enumerate(self.embeddings)]\n#         x = torch.cat(x, dim=1)\n#         x = self.emb_drop(x)\n#         x2 = self.bn1(x_num)\n#         x = torch.cat([x, x2], dim=1)\n#         x = F.relu(self.lin1(x))\n#         x = self.drops(x)\n#         x = self.bn2(x)\n#         x = F.relu(self.lin2(x))\n#         x = self.drops(x)\n#         x = self.bn3(x)\n#         x = self.lin3(x)\n#         return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.386294Z","iopub.status.idle":"2025-09-15T01:28:35.386662Z","shell.execute_reply.started":"2025-09-15T01:28:35.386528Z","shell.execute_reply":"2025-09-15T01:28:35.386539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ラベルエンコード済みカテゴリ変数の埋め込み\n# # 各カテゴリ列の変数の種類\n# cat_sizes = [len(df_train[col].cat.categories) for col in CATEGORICAL]\n\n# # (入力サイズ, 50と割る2の小さい方)でエンコード\n# emb_sizes = [(size, min(50, (size+1)//2)) for size in cat_sizes]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.387925Z","iopub.status.idle":"2025-09-15T01:28:35.388549Z","shell.execute_reply.started":"2025-09-15T01:28:35.388378Z","shell.execute_reply":"2025-09-15T01:28:35.388393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 記録用\n# hist = {\n#     'train_loss': [], 'train_auc': [],\n#     'val_loss': [], 'val_auc': []\n# }\n\n# # パラメータ\n# bs = 64 # バッチサイズ\n# EPOCHS = 5 # エポック\n# save_every = 1\n# FOLDS = 5 # FOLD数\n# LR=1e-3 # 学習率\n\n# patience = 3\n\n# # stratified KFoldの宣言\n# skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n# fold_results = []\n\n# val_results = []\n# test_results = []\n\n# # SKFによるデータ分割\n# for fold, (train_idx, val_idx) in enumerate(skf.split(df_train.drop(columns=TARGET), df_train[TARGET])):\n    \n#     print(f\"\\n========== Fold {fold+1} ==========\")\n\n#     # 学習データ\n#     X_train = df_train.drop(columns=TARGET).iloc[train_idx] \n#     y_train = df_train[TARGET].iloc[train_idx]\n\n#     # バリデーションデータ\n#     X_val = df_train.drop(columns=TARGET).iloc[val_idx]\n#     y_val = df_train[TARGET].iloc[val_idx]\n\n#     # Datasetの作成\n#     train_dataset = CustomDataset(X_train, y_train)\n#     val_dataset = CustomDataset(X_val, y_val)\n    \n#     # DataLoaderの作成\n#     train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n#     val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=0)\n\n#     # モデル構築\n#     model = NN_Model(emb_sizes, len(NUMERICAL)).to(device)\n\n#     # 最適化設定\n#     optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n#     # optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n\n#     # 損失関数\n#     criterion = nn.CrossEntropyLoss()\n\n#     hist = {\"train_auc\": [], \"val_auc\": []}\n#     best_val_auc = 0\n#     counter = 0\n\n#     # 学習・予測エポックのループ\n#     for epoch in range(EPOCHS):\n\n#         # 開始時間\n#         start_time = time.time()\n\n#         # 学習モード\n#         model.train()\n\n#         # ラベル、予測値の保存場所\n#         y_true_train, y_pred_train = [], []\n\n#         # プログレスバー\n#         train_iter = tqdm(train_loader, desc=f\"<Train> Epoch {epoch+1}\", leave=False)\n        \n#         for i, (cat_data, num_data, target) in enumerate(train_iter):\n\n#             # DataLoaderから取り出した、カテゴリ、数値、ターゲット\n#             cat_data, num_data, target = cat_data.to(device), num_data.to(device), target.to(device)\n\n#             # パラメータの勾配を初期化\n#             optimizer.zero_grad()\n\n#             # 予測値の算出\n#             output = model(cat_data, num_data)\n\n#             # ラベルと予測値とのロス計算\n#             loss = criterion(output, target)\n\n#             # 各パラメータの勾配を算出\n#             loss.backward()\n\n#             # パラメータ更新\n#             optimizer.step()\n\n#             # ソフトマックスの分類結果を格納\n#             probs = torch.softmax(output, dim=1)[:, 1].detach().cpu().numpy()\n#             y_pred_train.extend(probs)\n\n#             # ラベルの格納\n#             y_true_train.extend(target.cpu().numpy())\n\n#             # プログレスバーの後ろにロス値を表示\n#             if i % 10 == 0:\n#                 train_iter.set_postfix(loss=loss.item())\n\n#         # チェックポイント保存\n#         if (epoch + 1) % save_every == 0:\n#             torch.save(model.state_dict(), f\"model_epoch{epoch+1}.pt\")        \n    \n#         # histに残すAUCスコア\n#         train_auc = roc_auc_score(y_true_train, y_pred_train)\n\n#         # 評価モード\n#         model.eval()\n\n#         # ラベル、予測値の保存場所        \n#         y_true_val, y_pred_val = [], []\n\n#         # プログレスバー\n#         val_iter = tqdm(val_loader, desc=f\"<Val> Epoch {epoch+1}\", leave=False)\n\n#         # 勾配を更新しない\n#         with torch.no_grad():\n            \n#             for cat_data, num_data, target in val_iter:\n    \n#                 # DataLoaderから取り出した、カテゴリ、数値、ターゲット\n#                 cat_data, num_data, target = cat_data.to(device), num_data.to(device), target.to(device)\n\n#                 # 予測値の算出\n#                 output = model(cat_data, num_data)\n\n#                 # ソフトマックスの分類結果を格納\n#                 probs = torch.softmax(output, dim=1)[:, 1].cpu().numpy()\n#                 y_pred_val.extend(probs)\n\n#                 # ラベルの格納\n#                 y_true_val.extend(target.cpu().numpy())\n\n#                 # プログレスバーの後ろにロス値を表示\n#                 val_iter.set_postfix(loss=criterion(output, target).item())\n\n#         # histに残すAUCスコア        \n#         val_auc = roc_auc_score(y_true_val, y_pred_val)\n\n#         # 差分時刻\n#         elapsed = time.time() - start_time\n\n#         # 履歴追加\n#         hist[\"train_auc\"].append(train_auc)\n#         hist[\"val_auc\"].append(val_auc)\n\n#         # 進捗\n#         print(f\"Epoch {epoch+1}/{EPOCHS} - TrainAUC: {train_auc:.4f} | ValAUC: {val_auc:.4f} | Time: {elapsed:.1f}s\")\n\n#         # チェックポイント\n#         if (epoch + 1) % save_every == 0:\n#             torch.save(model.state_dict(), f\"model_fold{fold+1}_epoch{epoch+1}.pth\")\n\n#         # EarlyStopping判定\n#         if val_auc > best_val_auc:\n#             best_val_auc = val_auc\n#             counter = 0\n#             torch.save(model.state_dict(), f\"best_model_fold{fold+1}.pth\")\n#         else:\n#             counter += 1\n#             if counter >= patience:\n#                 print(f\"Early stopping at epoch {epoch+1}\")\n#                 break\n\n    \n#     # foldごとに保存\n#     torch.save(model.state_dict(), f\"model_fold{fold+1}.pth\")\n    \n#     # ヒストグラムの更新\n#     fold_results.append(hist)\n\n#     # foldごとにテストデータ計算\n#     model.eval()\n#     with torch.no_grad():\n#         X_val_cat = torch.from_numpy(df_train[CATEGORICAL].values.astype(np.int64)).to(device)\n#         X_val_num = torch.from_numpy(df_train[NUMERICAL].values.astype(np.float32)).to(device)\n\n#         # 予測\n#         preds = torch.softmax(model(X_val_cat, X_val_num),dim=1)[:,1].cpu().numpy()\n#         val_results.append(preds)\n        \n#         X_test_cat = torch.from_numpy(df_test[CATEGORICAL].values.astype(np.int64)).to(device)\n#         X_test_num = torch.from_numpy(df_test[NUMERICAL].values.astype(np.float32)).to(device)\n\n#         # 予測\n#         preds = torch.softmax(model(X_test_cat, X_test_num),dim=1)[:,1].cpu().numpy()\n#         # preds = torch.softmax(model(X_test_cat, X_test_num).squeeze()).cpu().numpy()\n#         test_results.append(preds)\n    \n# # shape = (n_folds, n_test_samples) → 平均化\n# val_results = np.mean(val_results, axis=0)        \n# test_results = np.mean(test_results, axis=0)        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.389508Z","iopub.status.idle":"2025-09-15T01:28:35.390213Z","shell.execute_reply.started":"2025-09-15T01:28:35.390027Z","shell.execute_reply":"2025-09-15T01:28:35.390042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pred_NN = val_results\n# pred_NN_test = test_results\n# AUC_NN = np.average(hist[\"val_auc\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.391380Z","iopub.status.idle":"2025-09-15T01:28:35.392046Z","shell.execute_reply.started":"2025-09-15T01:28:35.391880Z","shell.execute_reply":"2025-09-15T01:28:35.391894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ======================\n# # FoldごとのAUCをプロット\n# # ======================\n# import matplotlib.pyplot as plt\n# plt.figure(figsize=(10,5))\n# for i, hist in enumerate(fold_results):\n#     plt.plot(hist[\"val_auc\"], label=f\"Fold {i+1} Val AUC\")\n# plt.xlabel(\"Epoch\")\n# plt.ylabel(\"AUC\")\n# plt.legend()\n# plt.title(\"Validation AUC per Fold\")\n# plt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.392798Z","iopub.status.idle":"2025-09-15T01:28:35.393147Z","shell.execute_reply.started":"2025-09-15T01:28:35.392944Z","shell.execute_reply":"2025-09-15T01:28:35.392956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.ensemble import StackingClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import cross_val_score, StratifiedKFold\n# from sklearn.metrics import roc_auc_score\n\n# print(\"# STACKING ENSEMBLE IMPLEMENTATION\")\n# print(\"# \" + \"=\"*50)\n# print(\"# Combining top 4 models: LightGBM, XGBoost, CatBoost, NN\")\n# print(\"# Meta-learner: Logistic Regression\")\n# print(\"# \" + \"=\"*50)\n\n# stacking_train = pd.DataFrame({\n#     'lgb': pred_lgb,\n#     'xgb': pred_xgb, \n#     'cat': pred_cb,\n#     'NN': pred_NN,\n# })\n\n# stacking_test = pd.DataFrame({\n#     'lgb': pred_lgb_test,\n#     'xgb': pred_xgb_test,\n#     'cat': pred_cb_test,\n#     'NN': pred_NN_test,\n# })\n\n# print(f\"# Stacking train shape: {stacking_train.shape}\")\n# print(f\"# Stacking test shape: {stacking_test.shape}\")\n\n# print(\"\\n# METHOD 1: WEIGHTED AVERAGE\")\n# print(\"# \" + \"-\"*30)\n\n# scores = [AUC_lgb, AUC_xgb, AUC_cb, AUC_NN]  \n# total_score = sum(scores)\n# weights = [score/total_score for score in scores]\n\n# print(f\"# Model weights:\")\n# print(f\"# LightGBM: {weights[0]:.4f}\")\n# print(f\"# XGBoost:  {weights[1]:.4f}\")\n# print(f\"# CatBoost: {weights[2]:.4f}\")\n# print(f\"# NN: {weights[3]:.4f}\")\n\n# weighted_oof = (stacking_train['lgb'] * weights[0] + \n#                 stacking_train['xgb'] * weights[1] + \n#                 stacking_train['cat'] * weights[2] +\n#                 stacking_train['NN'] * weights[3])\n\n# weighted_test = (stacking_test['lgb'] * weights[0] + \n#                  stacking_test['xgb'] * weights[1] + \n#                  stacking_test['cat'] * weights[2] +\n#                  stacking_test['NN'] * weights[3])\n\n# weighted_score = roc_auc_score(y, weighted_oof)\n# print(f\"# Weighted Average ROC AUC: {weighted_score:.6f}\")\n\n# print(\"\\n# METHOD 2: LOGISTIC REGRESSION META-LEARNER\")\n# print(\"# \" + \"-\"*40)\n\n# meta_learner = LogisticRegression(penalty=\"l2\",random_state=42, max_iter=1000)\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# cv_scores = cross_val_score(meta_learner, stacking_train, y, \n#                            cv=skf, scoring='roc_auc', n_jobs=-1)\n\n# print(f\"# Meta-learner CV scores: {[f'{score:.6f}' for score in cv_scores]}\")\n# print(f\"# Meta-learner mean CV: {cv_scores.mean():.6f} ± {cv_scores.std():.6f}\")\n\n# meta_learner.fit(stacking_train, y)\n# meta_oof = meta_learner.predict_proba(stacking_train)[:, 1]\n# meta_test = meta_learner.predict_proba(stacking_test)[:, 1]\n# meta_score = roc_auc_score(y, meta_oof)\n\n# print(f\"# Meta-learner ROC AUC: {meta_score:.6f}\")\n\n# coefficients = meta_learner.coef_[0]\n# print(f\"# Meta-learner coefficients:\")\n# print(f\"# LightGBM: {coefficients[0]:.4f}\")\n# print(f\"# XGBoost:  {coefficients[1]:.4f}\")\n# print(f\"# CatBoost: {coefficients[2]:.4f}\")\n# print(f\"# NN: {coefficients[3]:.4f}\")\n# print(f\"# Intercept: {meta_learner.intercept_[0]:.4f}\")\n\n# print(\"\\n# METHOD 3: SIMPLE AVERAGE (BASELINE)\")\n# print(\"# \" + \"-\"*35)\n\n# simple_oof = (stacking_train['lgb'] + stacking_train['xgb'] + stacking_train['cat'] + stacking_train['NN']) / 4\n# simple_test = (stacking_test['lgb'] + stacking_test['xgb'] + stacking_test['cat'] + stacking_test['NN']) / 4\n# simple_score = roc_auc_score(y, simple_oof)\n\n# print(f\"# Simple Average ROC AUC: {simple_score:.6f}\")\n\n# print(\"\\n# ENSEMBLE METHODS COMPARISON\")\n# print(\"# \" + \"=\"*40)\n# ensemble_results = [\n#     ('Individual LightGBM', AUC_lgb),\n#     ('Individual XGBoost', AUC_xgb),\n#     ('Individual CatBoost', AUC_cb),\n#     ('Individual NN', AUC_NN),\n#     ('Weighted Average', weighted_score),\n#     ('Meta-learner (LogReg)', meta_score),\n#     ('Simple Average', simple_score)\n# ]\n\n# ensemble_results.sort(key=lambda x: x[1], reverse=True)\n\n# for i, (method, score) in enumerate(ensemble_results, 1):\n#     print(f\"# {i}. {method:<25}: {score:.6f}\")\n\n# best_method, best_score = ensemble_results[0]\n# print(f\"\\n# BEST ENSEMBLE METHOD: {best_method}\")\n# print(f\"# BEST ENSEMBLE SCORE: {best_score:.6f}\")\n\n# if 'Meta-learner' in best_method:\n#     final_oof = meta_oof\n#     final_test = meta_test\n#     print(\"# Using Meta-learner predictions for final submission\")\n# elif 'Weighted' in best_method:\n#     final_oof = weighted_oof\n#     final_test = weighted_test\n#     print(\"# Using Weighted Average predictions for final submission\")\n# else:\n#     final_oof = simple_oof\n#     final_test = simple_test\n#     print(\"# Using Simple Average predictions for final submission\")\n\n# print(\"\\n# STACKING ENSEMBLE COMPLETED!\")\n# print(\"# \" + \"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.393780Z","iopub.status.idle":"2025-09-15T01:28:35.394076Z","shell.execute_reply.started":"2025-09-15T01:28:35.393928Z","shell.execute_reply":"2025-09-15T01:28:35.393942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 提出データ作成\n# sample_submission = pd.read_csv(\"/kaggle/input/playground-series-s5e8/sample_submission.csv\")\n\n# sample_submission['y'] = test_results\n# sample_submission.to_csv('submission.csv', index=False)\n# print('Submission file saved.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T01:28:35.395216Z","iopub.status.idle":"2025-09-15T01:28:35.395521Z","shell.execute_reply.started":"2025-09-15T01:28:35.395375Z","shell.execute_reply":"2025-09-15T01:28:35.395388Z"}},"outputs":[],"execution_count":null}]}